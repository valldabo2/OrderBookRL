hpsearch-impala-aggressive:
    env: MarketOrderEnv-v0
    run: IMPALA
#    resources_per_trial:
#      cpu: 20
#      gpu: 0
    num_samples: 2
    stop:
        time_total_s: 600 # 60*60*10
    config:
        # Env configuration
        env_config:
          max_sequence_skip: 1500
          random_start: True
          max_episode_time: 2hours

        # Override model config
        model:
          # if Fully connected net
          fcnet_hiddens: [32, 32]
          # Free log std
          free_log_std: False
          # Whether to squash the action output to space range
          squash_to_range: True

          custom_preprocessor: mv
          custom_options:
            fast_macd_l: 120
            slow_macd_l: 240
            ofi_l: 100
            mid_l: 100
        # Which observation filter to apply to the observation
        observation_filter: MeanStdFilter

        clip_rewards: True
        # Whether to rollout "complete_episodes" or "truncate_episodes"
        batch_mode: truncate_episodes


        # V-trace params (see vtrace.py).
        vtrace: True
        vtrace_clip_rho_threshold: 1.0,
        vtrace_clip_pg_rho_threshold: 1.0,

        # System params.
        #
        # == Overview of data flow in IMPALA ==
        # 1. Policy evaluation in parallel across `num_workers` actors produces
        #    batches of size `sample_batch_size * num_envs_per_worker`.
        # 2. If enabled, the replay buffer stores and produces batches of size
        #    `sample_batch_size * num_envs_per_worker`.
        # 3. If enabled, the minibatch ring buffer stores and replays batches of
        #    size `train_batch_size` up to `num_sgd_iter` times per batch.
        # 4. The learner thread executes data parallel SGD across `num_gpus` GPUs
        #    on batches of size `train_batch_size`.
        #
        sample_batch_size: 2000,
        train_batch_size: 10000
        min_iter_time_s: 30
        num_workers: 2
        # number of GPUs the learner should use.
        num_gpus: 1
        # set >1 to load data into GPUs in parallel. Increases GPU memory usage
        # proportionally with the number of buffers.
        num_data_loader_buffers: 1
        # how many train batches should be retained for minibatching. This conf
        # only has an effect if `num_sgd_iter > 1`.
        minibatch_buffer_size: 1
        # number of passes to make over each train batch
        num_sgd_iter: 1
        # set >0 to enable experience replay. Saved samples will be replayed with
        # a p:1 proportion to new data samples.
        replay_proportion: 0.0
        # number of sample batches to store for replay. The number of transitions
        # saved total will be (replay_buffer_num_slots * sample_batch_size).
        replay_buffer_num_slots: 0
        # max queue size for train batches feeding into the learner
        learner_queue_size: 16
        # level of queuing for sampling.
        max_sample_requests_in_flight_per_worker: 2
        # max number of workers to broadcast one set of weights to
        broadcast_interval: 1

        # Learning params.
        grad_clip: 40.0
        # either "adam" or "rmsprop"
        opt_type: "adam"
        lr: 0.0005
        lr_schedule: None
        # rmsprop considered
        decay: 0.99
        momentum: 0.0
        epsilon: 0.1
        # balancing the three losses
        vf_loss_coeff: 0.5
        entropy_coeff: -0.01,

    local_dir: ~/Documents/Hobby/PythonProjects/orderbookrl/logs2/marketorderenv

