ppo:
    env: MarketOrderEnv-v0
    run: APPO
    stop:
        time_total_s:  36000 # 4*60*60
    config:
        env_config:
          max_sequence_skip: 1500
          random_start: True
          max_episode_time: 4hours

        # Model
        # If true, use the Generalized Advantage Estimator (GAE)
        vtrace: False
        use_gae: True
        clip_param: 0.3


        # Override model config
        model:
          # Whether to use LSTM model
          #use_lstm: True
          # Max seq length for LSTM training.
          max_seq_len: 100
          # Lstm cell size
          lstm_cell_size: 64
          # if Fully connected net
          fcnet_hiddens: [128, 128]
          # Free log std
          free_log_std: False
          # Whether to squash the action output to space range
          squash_to_range: True

          custom_preprocessor: mv
          custom_options:
            fast_macd_l: 1200
            slow_macd_l: 2400
            ofi_l: 1000
            mid_l: 1000
        # Which observation filter to apply to the observation
        observation_filter: MeanStdFilter

        # == IMPALA optimizer params (see documentation in impala.py) ==
        sample_batch_size: 5000
        train_batch_size: 20000
        min_iter_time_s: 10
        num_workers: 4
        num_gpus: 1
        num_data_loader_buffers: 1
        minibatch_buffer_size: 1
        num_sgd_iter: 10
        replay_proportion: 0.1
        replay_buffer_num_slots: 100
        max_sample_requests_in_flight_per_worker: 2
        broadcast_interval: 1
        grad_clip: 40.0
        opt_type: "adam"
        lr: 0.0005
        lr_schedule: None
        decay: 0.99
        momentum: 0.0
        epsilon: 0.1
        vf_loss_coeff: 0.5
        entropy_coeff: -0.01


    #local_dir: /media/olle/DATA/Projects/orderbookrl/logs/marketorderenvnofee
    checkpoint_freq: 10

