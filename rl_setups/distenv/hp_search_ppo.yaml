hpsearch-ppo:
    env: DistEnv-v0
    run: PPO
    stop:
        time_total_s: 30
    config:
        # Model
        # If true, use the Generalized Advantage Estimator (GAE)
        # with a value function, see https://arxiv.org/pdf/1506.02438.pdf.
        use_gae: True
        # GAE(lambda) parameter
        lambda: 1.0
        # Initial coefficient for KL divergence
        kl_coeff: 0.2
        # Override model config
        model:
          # Whether to use LSTM model
          use_lstm: False
          # Max seq length for LSTM training.
          max_seq_len: 20
          custom_preprocessor: mv
          custom_options:
            fast_macd_l: 1200
            slow_macd_l: 2400
            ofi_l: 1000
            mid_l: 100
        # Which observation filter to apply to the observation
        observation_filter: MeanStdFilter

        # Optimization
        # Number of timesteps collected for each SGD round
        timesteps_per_batch: 512
        # Number of SGD iterations in each outer loop
        num_sgd_iter:
          grid_search: [4, 8, 12]
        # Stepsize of SGD
        sgd_stepsize: 5e-5
        # Total SGD batch size across all devices for SGD (multi-gpu only)
        sgd_batchsize: 128
        # Coefficient of the value function loss
        vf_loss_coeff: 1.0
        # Coefficient of the entropy regularizer
        entropy_coeff: 0.0
        # PPO clip parameter
        clip_param: 0.3
        # Target value for KL divergence
        kl_target: 0.01
        # Use the sync samples optimizer instead of the multi-gpu one
        simple_optimizer: False

        # Resources
        num_workers: 1
        # Number of GPUs to use for SGD
        num_gpus: 0
        # Whether to allocate GPUs for workers (if > 0).
        num_gpus_per_worker: 0
        # Whether to allocate CPUs for workers (if > 0).
        num_cpus_per_worker: 1
        # Whether to rollout "complete_episodes" or "truncate_episodes"
        batch_mode: truncate_episodes

    local_dir: ~/Documents/OrderBookRL/logs/distenv

